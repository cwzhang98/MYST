{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f18497f-a5fd-449d-9b61-27471ee9b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1cc14-609d-409b-ad56-64d4efee6ad3",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f6688-eea5-433c-925e-a120f15bf1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.module):\n",
    "    def __init__(self, model_dim, num_heads, attn_dropout, proj_drop):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = model_dim // num_heads\n",
    "        self.scale = model_dim ** -0.5  # sqrt(d)\n",
    "        self.qkv_proj = nn.Linear(model_dim, model_dim * 3, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.proj = nn.Linear(model_dim, model_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        qkv = self.qkv_proj(x)  # B, T, D * 3\n",
    "        qkv = qkv.reshape(B, T, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4) # 3, B, num_heads, T, head_dim\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = torch.mm(q, k.transpose(-1, -2))\n",
    "        attn = attn * self.scale # B, num_heads, T, T\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        x = torch.mm(attn, v).transpose(1, 2).reshape(B, T, -1)  # B, num_heads, T, head_dim -> B, T, num_heads, head_dim -> B, T, D\n",
    "\n",
    "        return self.proj_drop(self.proj(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e40094-c4bd-4e04-84c1-368d9b988fdd",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b181315-c5c6-45eb-80bb-eb15dd1b1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.module):\n",
    "    \"\"\"\n",
    "    pe(2i) = sin(pos / 10000 ** (2i/ model_dim))\n",
    "    pe(2i+1) = cos(pos / 10000 ** (2i/ model_dim))\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dim, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arrange(0, max_len).unsqueeze(1)  # max_len * 1\n",
    "        div_term = torch.exp((-math.log(10000.0) / model_dim) * torch.arange(0, model_dim, 2).float())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # pe: (max_len, dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x): # B T C\n",
    "        x = x + self.pe[:x.size(1), :].unsqueeze(0)\n",
    "        return x\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st",
   "language": "python",
   "name": "st"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
